{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNKlC1CifGuUcFfbtXpRLfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmaacario/spatial-semantics-translation/blob/main/Error_Analysis_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas"
      ],
      "metadata": {
        "id": "bK8zNvJF9sVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext cudf.pandas"
      ],
      "metadata": {
        "id": "TzF-d4ov-7NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def draw_encoder_decoder_attention():\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "\n",
        "    # Draw the encoder box\n",
        "    enc_box = patches.FancyBboxPatch((0.1, 0.5), 0.1, 0.2, boxstyle=\"round,pad=0.1\", edgecolor=\"black\", facecolor=\"#add8e6\")\n",
        "    ax.add_patch(enc_box)\n",
        "    ax.text(0.15, 0.6, 'ENC', horizontalalignment='center', verticalalignment='center', fontsize=12, weight='bold')\n",
        "\n",
        "    # Draw the attention box\n",
        "    att_box = patches.FancyBboxPatch((0.35, 0.5), 0.1, 0.2, boxstyle=\"round,pad=0.1\", edgecolor=\"black\", facecolor=\"#add8e6\")\n",
        "    ax.add_patch(att_box)\n",
        "    ax.text(0.4, 0.6, 'ATT', horizontalalignment='center', verticalalignment='center', fontsize=12, weight='bold')\n",
        "\n",
        "    # Draw the decoder box\n",
        "    dec_box = patches.FancyBboxPatch((0.6, 0.5), 0.1, 0.2, boxstyle=\"round,pad=0.1\", edgecolor=\"black\", facecolor=\"#add8e6\")\n",
        "    ax.add_patch(dec_box)\n",
        "    ax.text(0.65, 0.6, 'DEC', horizontalalignment='center', verticalalignment='center', fontsize=12, weight='bold')\n",
        "\n",
        "    # Draw input text\n",
        "    ax.text(0.1, 0.3, r'$<it> \\, the \\, cat \\, on \\, the \\, mat$', horizontalalignment='center', verticalalignment='center', fontsize=12, color='black')\n",
        "\n",
        "    # Draw output text\n",
        "    ax.text(0.75, 0.7, r'$il$', horizontalalignment='center', verticalalignment='center', fontsize=12, color='black')\n",
        "    ax.text(0.9, 0.7, r'$gatto$', horizontalalignment='center', verticalalignment='center', fontsize=12, color='red')\n",
        "    ax.text(0.75, 0.3, r'$<it> \\, il$', horizontalalignment='center', verticalalignment='center', fontsize=12, color='black')\n",
        "\n",
        "    # Draw arrows between boxes\n",
        "    ax.annotate('', xy=(0.2, 0.6), xytext=(0.35, 0.6),\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
        "    ax.annotate('', xy=(0.45, 0.6), xytext=(0.6, 0.6),\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
        "\n",
        "    # Draw state and embedding arrows\n",
        "    ax.annotate('', xy=(0.45, 0.6), xytext=(0.45, 0.8),\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
        "    ax.annotate('', xy=(0.55, 0.8), xytext=(0.6, 0.6),\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
        "    ax.annotate('', xy=(0.75, 0.6), xytext=(0.9, 0.6),\n",
        "                arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
        "\n",
        "    # Draw word, embeddings, states labels\n",
        "    ax.text(0.95, 0.65, 'states', horizontalalignment='left', verticalalignment='center', fontsize=12, color='blue')\n",
        "    ax.text(0.95, 0.35, 'embeddings', horizontalalignment='left', verticalalignment='center', fontsize=12, color='blue')\n",
        "    ax.text(0.95, 0.1, 'words', horizontalalignment='left', verticalalignment='center', fontsize=12, color='blue')\n",
        "\n",
        "    # Set limits and hide axes\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Draw the first diagram\n",
        "draw_encoder_decoder_attention()\n"
      ],
      "metadata": {
        "id": "Vl_ag7RSoBsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAZktpv-TuCh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import annotate, xlab, ylab, ggplot, aes, geom_tile, scale_fill_gradient, geom_text, theme_gray, theme_minimal, theme, element_text, coord_fixed, scale_fill_identity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from plotnine import *\n",
        "from datetime import datetime\n",
        "\n",
        "# Data for the timeline\n",
        "data = pd.DataFrame({\n",
        "    'date': [\n",
        "        '2019-10', '2020-05', '2020-10', '2021-04', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-12',\n",
        "        '2022-01', '2022-02', '2022-03', '2022-04', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12',\n",
        "        '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08',\n",
        "        '2024-01'\n",
        "    ],\n",
        "    'model': [\n",
        "        'T5', 'GPT-3', 'mT5', 'PanGu-Î±', 'CPM-2', 'Codex', 'ERNIE 3.0', 'Jurassic-1', 'HyperCLOVA', 'Yuan 1.0', 'Gopher', 'ERNIE 3.0 Titan', 'GLAM', 'LaMDA', 'WebGPT',\n",
        "        'MT-NLG', 'AlphaCode', 'Chinchilla', 'PaLM', 'AlexaTM', 'Sparrow', 'U-PaLM', 'Flan-U-PaLM', 'BLOOM', 'ChatGPT',\n",
        "        'OPT-IML', 'mT0', 'Galactica', 'GLM', 'OPT', 'UL2', 'Tk-Instruct', 'GPT-NeoX-20B', 'CodeGen', 'PaLM 2', 'Claude', 'Bard', 'Gemini'\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Convert date to datetime\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "# Create the plot\n",
        "timeline_plot = (\n",
        "    ggplot(data, aes(x='date', y=0, label='model')) +\n",
        "    geom_point(color='black') +\n",
        "    geom_text(aes(label='model'), ha='left', nudge_y=0.3, size=8) +\n",
        "    geom_segment(aes(xend='date', yend=0), linetype='solid') +\n",
        "    scale_x_datetime(date_labels='%b\\n%Y', date_breaks='3 months') +\n",
        "    theme_minimal() +\n",
        "    theme(\n",
        "        axis_title_y=element_blank(),\n",
        "        axis_text_y=element_blank(),\n",
        "        axis_ticks_y=element_blank(),\n",
        "        panel_grid_major_y=element_blank(),\n",
        "        panel_grid_minor_y=element_blank(),\n",
        "        axis_text_x=element_text(size=10),\n",
        "        axis_title_x=element_text(size=12),\n",
        "        plot_margin={'top': 10, 'right': 50, 'bottom': 30, 'left': 50}\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "print(timeline_plot)"
      ],
      "metadata": {
        "id": "1r-KWu-_SeFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Generate data for the normal distribution\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = norm.pdf(x)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({'x': x, 'y': y})\n",
        "\n",
        "# Create the plot\n",
        "p = (\n",
        "    ggplot(data, aes(x='x', y='y')) +\n",
        "    geom_line(color='black') +\n",
        "    geom_ribbon(aes(ymin=0, ymax='y'), data=data[(x >= -1.96) & (x <= 1.96)], fill='lightblue') +\n",
        "    geom_ribbon(aes(ymin=0, ymax='y'), data=data[(x < -1.96)], fill='lightgray') +\n",
        "    geom_ribbon(aes(ymin=0, ymax='y'), data=data[(x > 1.96)], fill='lightgray') +\n",
        "    annotate('text', x=0, y=0.15, label='95% Confidence Level', ha='center', size=10) +\n",
        "    annotate('text', x=-3, y=0.06, label='Significance Level', ha='center', size=10) +\n",
        "    annotate('text', x=3, y=0.06, label='Significance Level', ha='center', size=10) +\n",
        "    annotate('text', x=-3, y=0.04, label='2%', ha='center', size=10) +\n",
        "    annotate('text', x=3, y=0.04, label='2%', ha='center', size=10) +\n",
        "    labs(x='Confidence Interval', y='') +\n",
        "    theme_void() +\n",
        "    theme(axis_text_y=element_blank(), axis_ticks_major_y=element_blank())\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "print(p)"
      ],
      "metadata": {
        "id": "dGMvCYdG9SnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from plotnine import ggplot, aes, geom_rect, geom_text, theme_void, theme\n",
        "\n",
        "# Data for the rectangles\n",
        "rects = pd.DataFrame({\n",
        "    'xmin': [0, 1, 0, 1],\n",
        "    'xmax': [1, 2, 1, 2],\n",
        "    'ymin': [1, 1, 0, 0],\n",
        "    'ymax': [2, 2, 1, 1],\n",
        "    'label': [\n",
        "        'moldura de satÃ©lite\\nâ\\nMANEIRA no verbo',\n",
        "        'moldura de verbo\\nâ\\nTRAJETO no verbo',\n",
        "        'INGLÃS\\nlimp, tip-toe, crawl',\n",
        "        'PORTUGUÃS\\nsubir, descer, entrar, sair'\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Create the plot\n",
        "p = (ggplot(rects) +\n",
        "     geom_rect(aes(xmin='xmin', xmax='xmax', ymin='ymin', ymax='ymax'),\n",
        "               fill=None, color='black', size=1) +  # Border size\n",
        "     geom_text(aes(x='(xmin + xmax) / 2', y='(ymin + ymax) / 2', label='label'),\n",
        "               ha='center', va='center', size=15, fontstyle='italic') +  # Text size\n",
        "     theme_void() +\n",
        "     theme(plot_margin=0)  # Remove plot margins\n",
        "    )\n",
        "\n",
        "# Display the plot\n",
        "print(p)"
      ],
      "metadata": {
        "id": "5Hih3vUDTb2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = ['cc', 'un', 'om', 'ad', 'ag', 'co',\n",
        "         'wl', 'wt', 'an', 'gr', 'in', 'ie']\n",
        "\n",
        "s_errors = ['sp', 'po', 'ws']\n",
        "\n",
        "ie_errors = ['ie']\n",
        "\n",
        "ns_errors = ['un', 'om', 'ad', 'ag', 'co',\n",
        "              'wl', 'wt', 'an', 'gr', 'in', 'ie']\n",
        "\n",
        "error_types_to_include = list(set(s_errors + errors) - {'cc'})"
      ],
      "metadata": {
        "id": "0Uvoglnw9jBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ns_errors"
      ],
      "metadata": {
        "id": "9TKNXqE6Ahb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_labels = {\n",
        "        'gemma:7b': 'Gemma-7B',\n",
        "        'llama2:7b': 'LLaMA-2-7B',\n",
        "        'llama2:13b': 'LLaMA-2-13B',\n",
        "        'llama3': 'LLaMA-3-8B',\n",
        "        'deepl': 'DeepL',\n",
        "        'mistral': 'Mistral-7B',\n",
        "        'amazon-stock': 'Amazon (Stock)',\n",
        "        'amazon-custom': 'Amazon (Custom)',\n",
        "        'googletrans': 'Google',\n",
        "        'mixtral': 'Mixtral-8x7B'\n",
        "    }"
      ],
      "metadata": {
        "id": "EYTo9kR6e0HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#folder_path = \"/content/Untitled Folder\"\n",
        "\n",
        "df = pd.read_csv('/content/Mixtral-8x7b_merged.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "O8Z2Bf-G7p59",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff = df[df['inner_id'] == 2797]\n",
        "df\n",
        "\n",
        "print(dff['reference'].iloc[0])"
      ],
      "metadata": {
        "id": "IbCyh48lLtmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/TEDTalks.en_pt-br.Mixtral.ANALYZED.csv')\n",
        "df_filtered = df[df['spatial_sense'].str.contains('Onto') & (df['error_type'] == 'ie')]\n",
        "df_filtered['inner_id'].iloc[0]"
      ],
      "metadata": {
        "id": "zgeWGgvalwb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'content/'"
      ],
      "metadata": {
        "id": "Fq4AuOjkTjE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to split the 'spatial_sense' column\n",
        "def extract_spatial_sense_label(spatial_sense):\n",
        "    return spatial_sense.split('(')[0]"
      ],
      "metadata": {
        "id": "GAQU5h2NGaEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def process_folder(folder_path, column, model_labels):\n",
        "    # Get a list of all files with .csv extension in the folder\n",
        "    file_paths = glob.glob(folder_path + \"/*.csv\")\n",
        "\n",
        "    # Initialize an empty dictionary to store DataFrames\n",
        "    grouped_dfs = {}\n",
        "    model_names = []\n",
        "\n",
        "    # Iterate over each file in the folder\n",
        "    for file_path in file_paths:\n",
        "        print(\"Processing file:\", file_path)  # Debugging: Print the file being processed\n",
        "        # Read the file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Extract model name\n",
        "        model_name = df.columns[3] if len(df.columns) > 3 else None\n",
        "        print(\"Model name:\", model_name)  # Debugging: Print the extracted model name\n",
        "        if model_name:\n",
        "            model_names.append(model_name)\n",
        "\n",
        "        # Process error types\n",
        "        df['error_type'] = df['error_type'].str.split(',')\n",
        "        df['error_type'] = df['error_type'].apply(lambda x: [item.strip() for item in x if isinstance(item, str) and item.strip().isalpha()] if isinstance(x, list) and all(isinstance(item, str) for item in x) else [])\n",
        "        df = df.explode('error_type')\n",
        "        df['error_type'] = df['error_type'].replace({'ha': 'in', 'wo': 'gr', 're': 'gr', 'lt': 'an'})  # Replace error types as specified\n",
        "        df = df[df['error_type'].isin(errors) | df['error_type'].isin(s_errors)]  # Use '|' for element-wise logical OR operation\n",
        "\n",
        "        # Filter out rows where 'spatial_sense' ends with (5) or (3) if values are 'sp', 'po', 'ws'\n",
        "        df = df[~(df['spatial_sense'].str.endswith('(5)') | df['spatial_sense'].str.endswith('(3)')) | ~df['spatial_sense'].isin(['sp', 'po', 'ws'])]\n",
        "\n",
        "        # Check if 'Model' column exists\n",
        "        if 'Model' not in df.columns:\n",
        "            # Add a new column 'Model' with the model name\n",
        "            df['Model'] = model_name\n",
        "\n",
        "        # Apply the function to create a new column 'spatial_sense_label'\n",
        "        df['spatial_sense_label'] = df['spatial_sense'].apply(extract_spatial_sense_label)\n",
        "\n",
        "        # Group by the specified column and error type, and calculate counts\n",
        "        grouped_df = df.groupby([column, 'error_type', 'spatial_sense_label']).size().reset_index(name='count')\n",
        "\n",
        "        # Store the grouped DataFrame in the dictionary\n",
        "        grouped_dfs[model_name] = grouped_df\n",
        "\n",
        "    # Get unique values in the 'spatial_sense_label' column from the last DataFrame\n",
        "    unique_labels = df['spatial_sense_label'].unique()\n",
        "\n",
        "    return grouped_dfs, model_names, unique_labels"
      ],
      "metadata": {
        "id": "Jy52bhc6qowC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {\n",
        "    'spatial_sense': ['Across(3)', 'Across(5)', 'Across(1)', '3', '2'],\n",
        "    'error_type': ['sp', 'po', 'ws', 'other', 'sp']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "combined_df = pd.groupby(['spatial_sense', 'error_type'], as_index=False).sum()\n",
        "\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "UJ_1jI4dZyjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Directory containing the CSV files\n",
        "folder_path = '/content/'\n",
        "\n",
        "error_types = ['sp', 'ws', 'po']\n",
        "\n",
        "# Read and concatenate all CSV files in the folder\n",
        "all_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "df_list = [pd.read_csv(os.path.join(folder_path, file)) for file in all_files]\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Ensure the 'count' column exists\n",
        "if 'count' not in combined_df.columns:\n",
        "    combined_df['count'] = 1  # Assign a default count of 1 if not present\n",
        "\n",
        "# Update 'error_type' based on 'spatial_sense'\n",
        "combined_df.loc[combined_df['spatial_sense'].isin(['Into(3)', 'Onto(3)', 'Across(5)', 'Through(5)']) &\n",
        "                combined_df['error_type'].isin(error_types), 'error_type'] = 'ie'\n",
        "\n",
        "combined_df.loc[(combined_df['spatial_sense'].str.endswith('(1)') |\n",
        "                 combined_df['spatial_sense'].str.endswith('(2)') |\n",
        "                 combined_df['spatial_sense'].str.endswith('(4)') |\n",
        "                 combined_df['spatial_sense'].isin(['Across(3)', 'Through(3)'])) &\n",
        "                (combined_df['error_type'] == 'ie'), 'error_type'] = 'sp'\n",
        "\n",
        "# Aggregate counts based on the updated 'error_type'\n",
        "aggregated_df = combined_df.groupby(['spatial_sense', 'error_type'], as_index=False)['count'].sum()\n",
        "\n",
        "# Separate the aggregated data into different DataFrames for each error type\n",
        "df_sp = aggregated_df[aggregated_df['error_type'] == 'sp']\n",
        "df_ws = aggregated_df[aggregated_df['error_type'] == 'ws']\n",
        "df_po = aggregated_df[aggregated_df['error_type'] == 'po']\n",
        "\n",
        "# Group by 'spatial_sense' and sum the counts for each error type\n",
        "grouped_sp = df_sp.groupby('spatial_sense')['count'].sum().reset_index(name='count_sp')\n",
        "grouped_ws = df_ws.groupby('spatial_sense')['count'].sum().reset_index(name='count_ws')\n",
        "grouped_po = df_po.groupby('spatial_sense')['count'].sum().reset_index(name='count_po')\n",
        "\n",
        "# Merge the grouped DataFrames to get the total counts by 'spatial_sense'\n",
        "total_counts = pd.merge(grouped_sp, grouped_ws, on='spatial_sense', how='outer')\n",
        "total_counts = pd.merge(total_counts, grouped_po, on='spatial_sense', how='outer')\n",
        "\n",
        "# Fill NaN values with 0 and calculate the total count\n",
        "total_counts = total_counts.fillna(0)\n",
        "total_counts['total_count'] = total_counts['count_sp'] + total_counts['count_ws'] + total_counts['count_po']\n",
        "\n",
        "# Output the results\n",
        "print(\"Grouped DataFrame for SP:\")\n",
        "print(grouped_sp)\n",
        "print(\"\\nGrouped DataFrame for WS:\")\n",
        "print(grouped_ws)\n",
        "print(\"\\nGrouped DataFrame for PO:\")\n",
        "print(grouped_po)\n",
        "print(\"\\nTotal Counts by Spatial Sense:\")\n",
        "print(total_counts)"
      ],
      "metadata": {
        "id": "1Xv8fn0dUWVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_grouped_dfs(grouped_dfs, unique_labels, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for label in unique_labels:\n",
        "        combined_df = pd.concat([df for df in grouped_dfs.values() if not df.empty], ignore_index=True)\n",
        "        df_filtered = combined_df[combined_df['spatial_sense_label'] == label]\n",
        "        if not df_filtered.empty:\n",
        "            file_path = os.path.join(output_dir, f\"{label}_errors_corrects.csv\")\n",
        "            df_filtered.to_csv(file_path)\n",
        "            print(f\"Exported {label}_errors_corrects.csv to {output_dir}\")\n",
        "        else:\n",
        "            print(f\"No data found for label: {label}\")"
      ],
      "metadata": {
        "id": "HD7q63PW7Z8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df, model_names, unique_labels = process_folder(folder_path, 'spatial_sense', model_labels)\n",
        "export_grouped_dfs(grouped_df, unique_labels, 'content/output')\n",
        "grouped_df"
      ],
      "metadata": {
        "id": "rnOqByXm_NLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df"
      ],
      "metadata": {
        "id": "opAizutV1Sqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_export_total_error_corrects(df, column, model_labels, model_names, error_types_to_include):\n",
        "    # Filter rows containing 'cc'\n",
        "    filtered_df1 = df[df['error_type'].str.contains('cc')]\n",
        "\n",
        "    # Filter rows not containing 'cc'\n",
        "    filtered_df2 = df[~df['error_type'].str.contains('cc')]\n",
        "\n",
        "    # Create pivot tables for both filtered DataFrames\n",
        "    pivot1 = pd.pivot_table(filtered_df1, index=column, columns='error_type', values='count', aggfunc='sum', fill_value=0)\n",
        "    pivot2 = pd.pivot_table(filtered_df2, index=column, columns='error_type', values='count', aggfunc='sum', fill_value=0)\n",
        "\n",
        "    # Sum up all error columns from pivot2 into one column 'Total of Errors'\n",
        "    pivot2_total_errors = pivot2.sum(axis=1).to_frame('Total of Errors')\n",
        "\n",
        "    # Merge pivot2 and pivot2_total_errors\n",
        "    merged_pivot2 = pd.concat([pivot2, pivot2_total_errors], axis=1)\n",
        "\n",
        "    # Combine pivot tables to get the final result\n",
        "    model_counts = pivot1.add(merged_pivot2, fill_value=0)\n",
        "\n",
        "    # Calculate Total of Corrects\n",
        "    model_counts['Total of Corrects'] = model_counts['cc']\n",
        "\n",
        "    # Calculate Percentage of Corrects\n",
        "    model_counts['% of Corrects'] = (model_counts['cc'] / (model_counts['cc'] + model_counts['Total of Errors'])) * 100\n",
        "\n",
        "    # Calculate Total of Errors\n",
        "    model_counts['Total of Errors'] = model_counts.loc[:, error_types_to_include].sum(axis=1)\n",
        "\n",
        "    # Calculate Percentage of Errors\n",
        "    model_counts['% of Errors'] = 100 - model_counts['% of Corrects']\n",
        "\n",
        "    # Substitute model labels directly into the index\n",
        "    model_counts.index = model_counts.index.map(model_labels)\n",
        "\n",
        "    return model_counts"
      ],
      "metadata": {
        "id": "Uyiy_bj_pPv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_pivot_tables(model_counts, column, model_labels, model_names, errors, s_errors, ns_errors):\n",
        "\n",
        "    # Pivot table with total of corrects, total of errors, percentage of corrects, and percentage of total errors\n",
        "    pivot_total = model_counts.copy()\n",
        "\n",
        "    # Filter columns in model_counts based on spatial error types\n",
        "    spatial_errors = model_counts[s_errors]\n",
        "\n",
        "    # Filter columns in model_counts based on non-spatial error types\n",
        "    non_spatial_errors = model_counts[ns_errors]\n",
        "\n",
        "    pivot_spatial_nonspatial = pd.DataFrame(index=model_counts.index)\n",
        "    pivot_spatial_nonspatial['Total of Spatial Errors'] = spatial_errors.sum(axis=1)\n",
        "    pivot_spatial_nonspatial['Total of Non-Spatial Errors'] = non_spatial_errors.sum(axis=1)\n",
        "\n",
        "    pivot_spatial_nonspatial['Total of Errors'] = pivot_total['Total of Errors']\n",
        "    pivot_spatial_nonspatial['% of Spatial Errors'] = ((pivot_spatial_nonspatial['Total of Spatial Errors'] / pivot_spatial_nonspatial['Total of Errors']) * 100).round(2).astype(str) + '%'\n",
        "    pivot_spatial_nonspatial['% of Non-Spatial Errors'] = ((pivot_spatial_nonspatial['Total of Non-Spatial Errors'] / pivot_spatial_nonspatial['Total of Errors']) * 100).round(2).astype(str) + '%'\n",
        "\n",
        "    pivot_total['Total of Corrects'] = pivot_total['Total of Corrects'].astype(str).str.rstrip('.0')\n",
        "    pivot_total['Total of Errors'] = pivot_total['Total of Errors'].astype(str).str.rstrip('.0')\n",
        "    pivot_total['% of Corrects'] = pivot_total['% of Corrects'].round(2).astype(str) + '%'\n",
        "    pivot_total['% of Errors'] = pivot_total['% of Errors'].round(2).astype(str) + '%'\n",
        "\n",
        "    # Export pivot tables to CSV files\n",
        " #   pivot_total.to_csv('pivot_total.csv')\n",
        "  #  pivot_spatial_nonspatial.to_csv('pivot_spatial_nonspatial.csv')\n",
        "\n",
        "    return pivot_total, pivot_spatial_nonspatial"
      ],
      "metadata": {
        "id": "XGc72wYE4ujW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_export_total_error_corrects2(df, column, model_labels, model_names, error_types_to_include):\n",
        "    grouped_dfs = {}\n",
        "    unique_labels = df[column].unique()\n",
        "    print(\"Unique Labels:\", unique_labels)\n",
        "\n",
        "    for label in unique_labels:\n",
        "        filtered_df = df[df[column].str.contains(unique_labels)]\n",
        "        print(f\"Filtered DataFrame for {label}:\", filtered_df)\n",
        "\n",
        "        if not filtered_df.empty:\n",
        "            pivot_table = pd.pivot_table(filtered_df, index='spatial_sense', columns='error_type', values='count', aggfunc='sum', fill_value=0)\n",
        "            pivot_table['Total of Errors'] = pivot_table[error_types_to_include].sum(axis=1)\n",
        "            pivot_table['Total of Corrects'] = pivot_table['cc'] if 'cc' in pivot_table.columns else 0\n",
        "            pivot_table['% of Corrects'] = (pivot_table['Total of Corrects'] / (pivot_table['Total of Corrects'] + pivot_table['Total of Errors']) * 100).round(2)\n",
        "            pivot_table['% of Errors'] = 100 - pivot_table['% of Corrects']\n",
        "\n",
        "            pivot_table.index = pivot_table.index.map(lambda x: next((label for label in model_labels if label in x), x))\n",
        "\n",
        "            grouped_dfs[label] = pivot_table\n",
        "\n",
        "    return grouped_dfs"
      ],
      "metadata": {
        "id": "_ula9wPrfKun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_pivot_tables2(model_counts, column, model_labels, model_names, errors, s_errors, ns_errors):\n",
        "    pivot_tables = {}\n",
        "\n",
        "    # Get unique non-NaN values in the spatial_sense_label column\n",
        "    unique_labels = model_counts.index.dropna().unique()\n",
        "    # Include NaN as a possible spatial sense label\n",
        "    unique_labels = unique_labels.append(pd.Index([np.nan]))\n",
        "\n",
        "    # Iterate over unique values in the spatial_sense_label column\n",
        "    for spatial_label in unique_labels:\n",
        "        # Filter model counts for the current spatial_sense_label\n",
        "        if pd.isna(spatial_label):\n",
        "            filtered_model_counts = model_counts[model_counts.index.isna()]\n",
        "        else:\n",
        "            filtered_model_counts = model_counts[model_counts.index == spatial_label]\n",
        "\n",
        "        # Pivot table with total of corrects, total of errors, percentage of corrects, and percentage of total errors\n",
        "        pivot_total = filtered_model_counts.copy()\n",
        "\n",
        "        # Filter columns in model_counts based on spatial error types\n",
        "        spatial_errors = filtered_model_counts[s_errors]\n",
        "\n",
        "        # Filter columns in model_counts based on non-spatial error types\n",
        "        non_spatial_errors = filtered_model_counts[ns_errors]\n",
        "\n",
        "        pivot_spatial_nonspatial = pd.DataFrame(index=filtered_model_counts.index)\n",
        "        pivot_spatial_nonspatial['Total of Spatial Errors'] = spatial_errors.sum(axis=1)\n",
        "        pivot_spatial_nonspatial['Total of Non-Spatial Errors'] = non_spatial_errors.sum(axis=1)\n",
        "\n",
        "        pivot_spatial_nonspatial['Total of Errors'] = pivot_total['Total of Errors']\n",
        "        pivot_spatial_nonspatial['% of Spatial Errors'] = ((pivot_spatial_nonspatial['Total of Spatial Errors'] / pivot_spatial_nonspatial['Total of Errors']) * 100).round(2).astype(str) + '%'\n",
        "        pivot_spatial_nonspatial['% of Non-Spatial Errors'] = ((pivot_spatial_nonspatial['Total of Non-Spatial Errors'] / pivot_spatial_nonspatial['Total of Errors']) * 100).round(2).astype(str) + '%'\n",
        "\n",
        "        pivot_total['Total of Corrects'] = pivot_total['Total of Corrects'].astype(str).str.rstrip('.0')\n",
        "        pivot_total['Total of Errors'] = pivot_total['Total of Errors'].astype(str).str.rstrip('.0')\n",
        "        pivot_total['% of Corrects'] = pivot_total['% of Corrects'].round(2).astype(str) + '%'\n",
        "        pivot_total['% of Errors'] = pivot_total['% of Errors'].round(2).astype(str) + '%'\n",
        "\n",
        "        # Save pivot tables for each spatial_sense_label\n",
        "        pivot_tables[spatial_label] = (pivot_total, pivot_spatial_nonspatial)\n",
        "\n",
        "    return pivot_tables"
      ],
      "metadata": {
        "id": "tnJLMSFsakeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_with_spatial_sense(folder_path):\n",
        "    # Process and group error types using 'spatial_sense'\n",
        "    grouped_errors, spatial_sense_values = process_folder(folder_path, 'spatial_sense', model_labels)\n",
        "    # print(grouped_errors, spatial_sense_values)\n",
        "\n",
        "    # Extract unique spatial_sense labels\n",
        "    spatial_sense_labels = list(set([extract_spatial_sense_label(value) for value in spatial_sense_values]))\n",
        "\n",
        "    # Filter and export error types\n",
        "    filtered_errors = filter_and_export_total_error_corrects(grouped_errors, 'spatial_sense', spatial_sense_labels, spatial_sense_values, error_types_to_include)\n",
        "    # print(filtered_errors)\n",
        "\n",
        "    pivot_tables = export_pivot_tables(filtered_errors, 'spatial_sense', spatial_sense_labels, spatial_sense_values, errors, s_errors, ns_errors)\n",
        "    print(pivot_tables)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content\"  # Change this to your folder path\n",
        "    main_with_spatial_sense(folder_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "7sJE1LARRBwK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(folder_path):\n",
        "    # Process and group error types\n",
        "    grouped_df, model_names, unique_labels = process_folder(folder_path, 'spatial_sense', model_labels)\n",
        "    print(grouped_df, unique_labels)\n",
        "\n",
        "    # Filter and export error types\n",
        " #   filtered_errors = filter_and_export_total_error_corrects(grouped_df, 'spatial_sense', model_labels, model_names, error_types_to_include)\n",
        " #   print(filtered_errors)\n",
        "\n",
        "   # pivot_total, pivot_spatial_nonspatial = export_pivot_tables(filtered_errors, 'spatial_sense', model_labels, model_names, errors, s_errors, ns_errors)\n",
        "  #  print(pivot_total, pivot_spatial_nonspatial)\n",
        "    output_dir = \"/content/output\"\n",
        "    export_grouped_dfs(grouped_df, unique_labels, output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content\"  # Change this to your folder path\n",
        "    main(folder_path)"
      ],
      "metadata": {
        "id": "qu0rQq_2ojII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv(\"/content/pivot_total.csv\")\n",
        "\n",
        "#df = df[['Model', 'ad', 'ag', 'an', 'co', 'gr', 'ie', 'in', 'om', 'po', 'sp', 'un', 'wl', 'ws', 'wt']]\n",
        "#df"
      ],
      "metadata": {
        "id": "4tGhXHpmUAyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "\n",
        "def plot_heatmap(df, label):\n",
        "    \"\"\"\n",
        "    Plots a heatmap based on the provided DataFrame.\n",
        "    \"\"\"\n",
        "    error_substitutions = {\n",
        "        \"cc\": \"Correct\",\n",
        "        \"un\": \"Untranslated\",\n",
        "        \"om\": \"Omission\",\n",
        "        \"re\": \"Repetition\",\n",
        "        \"ad\": \"Addition\",\n",
        "        \"ag\": \"Agreement\",\n",
        "        \"co\": \"Collocation\",\n",
        "        \"wl\": \"Wrong Lexis\",\n",
        "        \"wt\": \"Wrong Mood/Tense\",\n",
        "        \"an\": \"Anglicism\",\n",
        "        \"gr\": \"Wrong Grammar\",\n",
        "        \"in\": \"Interlanguage\",\n",
        "        \"sp\": \"Syntactic Proj.\",\n",
        "        \"po\": \"Polysemy\",\n",
        "        \"ws\": \"Wrong Sense\",\n",
        "        \"ie\": \"Idiomatic Exp.\"\n",
        "    }\n",
        "\n",
        "    model_labels = {\n",
        "        'gemma:7b': 'Gemma-7B',\n",
        "        'llama2:7b': 'LLaMA-2-7B',\n",
        "        'llama2:13b': 'LLaMA-2-13B',\n",
        "        'llama3': 'LLaMA-3-8B',\n",
        "        'deepl': 'DeepL',\n",
        "        'mistral': 'Mistral-7B',\n",
        "        'amazon-stock': 'Amazon (Stock)',\n",
        "        'amazon-custom': 'Amazon (Custom)',\n",
        "        'googletrans': 'Google',\n",
        "        'mixtral': 'Mixtral-8x7B'\n",
        "    }\n",
        "\n",
        "    label_name = \"Meaning\" if label == \"spatial_sense\" else \"Model\"\n",
        "\n",
        "    # Preprocess dataframe to replace model labels\n",
        "    if 'Model' in df.columns:\n",
        "        df['Model'] = df['Model'].replace(model_labels)\n",
        "\n",
        "    if 'error_type' in df.columns:\n",
        "        df['error_type'] = df['error_type'].replace(error_substitutions)\n",
        "\n",
        "\n",
        "    p = (\n",
        "        ggplot(df, aes(x='error_type', y='spatial_sense'))\n",
        "        + geom_tile(aes(fill='count'), color=\"black\")\n",
        "        + geom_text(aes(label='count'), color=\"white\", size=8)  # Add labels to tiles\n",
        "        + scale_fill_gradient(low=\"lightblue\", high=\"darkblue\")\n",
        "        + coord_fixed()  # Fix aspect ratio\n",
        "        + theme_gray()\n",
        "        + theme(\n",
        "            figure_size=(10, 6),  # Adjust figure size\n",
        "            axis_text_x=element_text(angle=45, vjust=1, hjust=-5, size=8),  # Rotate x-axis labels\n",
        "            legend_position='right',  # Position of legend\n",
        "            legend_direction='vertical',  # Direction of legend\n",
        "            legend_title=element_text(size=10),  # Legend title size\n",
        "            legend_text=element_text(size=8),  # Legend text size\n",
        "        )\n",
        "        + xlab(\"Error Type\") + ylab(label_name)  # Set the axis labels\n",
        "    )\n",
        "\n",
        "    return print(p)"
      ],
      "metadata": {
        "id": "YWsFvp1Tqvp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyexcel_ods"
      ],
      "metadata": {
        "id": "kwBHIsgDjmw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyexcel_ods\n",
        "\n",
        "# Read data from ODS file\n",
        "data = pyexcel_ods.get_data('/content/bbb.ods')\n",
        "\n",
        "# Extract data from the first sheet\n",
        "df = pd.DataFrame(data['Sheet1'])\n",
        "\n",
        "# Transpose the DataFrame\n",
        "df = df.T\n",
        "\n",
        "# Set the first row as the header\n",
        "df.columns = df.iloc[0]\n",
        "\n",
        "# Drop the first row after using it as the header\n",
        "df = df[1:].reset_index(drop=True)\n",
        "\n",
        "#df['spatial_sense'] = df['spatial_sense'].astype(str)\n",
        "\n",
        "import re\n",
        "\n",
        "#df['spatial_sense_label'] = df['spatial_sense'].apply(lambda x: re.split(r'\\(|\\)', x)[0].strip())\n",
        "\n",
        "#df.to_csv('/content/bb.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "netRHd-wzBeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "#df.drop(df.columns[0], axis=1, inplace=True)  # Drop the first column\n",
        "# Assuming df is your DataFrame\n",
        "#df = df[~df['preposition'].isin(['TOTAL', 'None'])]\n",
        "df"
      ],
      "metadata": {
        "id": "9O9hHgCqlfzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "import pandas as pd\n",
        "\n",
        "def plot_stacked_bar(df):\n",
        "    \"\"\"\n",
        "    Plots a stacked bar chart based on the provided DataFrame.\n",
        "    \"\"\"\n",
        "    # Melt the DataFrame to have a long format suitable for plotting with plotnine\n",
        "    melted_df = pd.melt(\n",
        "        df,\n",
        "        id_vars=['preposition', 'error_type'],  # Include 'preposition' and 'error_type' columns as id_vars\n",
        "        value_vars=['error_count', 'correct_count'],\n",
        "        var_name='type',\n",
        "        value_name='count'\n",
        "    )\n",
        "\n",
        "    # Calculate total counts (errors + corrects) for each preposition and error type\n",
        "    melted_df['total_count'] = melted_df.groupby(['preposition', 'error_type'])['count'].transform('sum')\n",
        "\n",
        "    # Calculate percentages\n",
        "    melted_df['percentage'] = (melted_df['count'] / melted_df['total_count']) * 100\n",
        "\n",
        "    # Round percentages and convert to integers\n",
        "    melted_df['percentage'] = melted_df['percentage'].apply(lambda x: round(x)).astype(int)\n",
        "\n",
        "    melted_df = melted_df[melted_df['preposition'] == 'Total']\n",
        "\n",
        "    print(melted_df)\n",
        "\n",
        "    # Create the plot for prepositions\n",
        "    preposition_plot = (\n",
        "        ggplot(melted_df, aes(x='error_type', y='percentage', fill='type'))\n",
        "        + geom_bar(position=\"stack\", stat=\"identity\")\n",
        "        + geom_text(\n",
        "            aes(label='percentage.astype(str) + \"%\"'),\n",
        "            position=position_stack(vjust=0.5),  # Adjust position to stack\n",
        "            size=12,  # Adjust the text size\n",
        "            color=\"white\"\n",
        "        )\n",
        "        + scale_fill_manual(values=[\"lightblue\", \"darkblue\"], labels=[\"Correct\", \"Error\"])\n",
        "        + theme_gray()\n",
        "        + theme(\n",
        "            figure_size=(10, 6),\n",
        "            legend_position='right',\n",
        "            legend_direction='vertical',\n",
        "            legend_title=element_text(text='Type', size=12),\n",
        "            legend_text=element_text(size=10),\n",
        "            axis_text_x=element_text(size=10, weight = 'bold'),  # Adjust x-axis text size\n",
        "            axis_text_y=element_text(size=10),  # Adjust y-axis text size\n",
        "            axis_title_x=element_text(size=12),  # Adjust x-axis title size and weight\n",
        "            axis_title_y=element_text(size=12)  # Adjust y-axis title size and weight\n",
        "        )\n",
        "        + labs(x=\"Error Type\", y=\"Percentage\", )\n",
        "        + facet_wrap('~preposition', scales='free_y', ncol=2)  # Separate by preposition\n",
        "    )\n",
        "\n",
        "    print(preposition_plot)\n",
        "\n",
        "# Call the function with the example DataFrame\n",
        "plot_stacked_bar(df)\n"
      ],
      "metadata": {
        "id": "wgoPKsDXItVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "import pandas as pd\n",
        "\n",
        "def plot_stacked_bar(df):\n",
        "    \"\"\"\n",
        "    Plots individual stacked bar charts for each unique value of the 'preposition' column.\n",
        "    \"\"\"\n",
        "    # Melt the DataFrame to have a long format suitable for plotting with plotnine\n",
        "    melted_df = pd.melt(\n",
        "        df,\n",
        "        id_vars=['preposition', 'spatial_sense', 'error_type'],  # Include 'preposition' and 'error_type' columns as id_vars\n",
        "        value_vars=['error_count', 'correct_count'],\n",
        "        var_name='type',\n",
        "        value_name='count'\n",
        "    )\n",
        "\n",
        "    # Calculate total counts (errors + corrects) for each preposition and error type\n",
        "    melted_df['total_count'] = melted_df.groupby(['preposition', 'spatial_sense', 'error_type'])['count'].transform('sum')\n",
        "\n",
        "    # Calculate percentages\n",
        "    melted_df['percentage'] = (melted_df['count'] / melted_df['total_count']) * 100\n",
        "\n",
        "    # Round percentages and convert to integers\n",
        "    melted_df['percentage'] = melted_df['percentage'].apply(lambda x: round(x)).astype(int)\n",
        "\n",
        "    # Filter out 'Total' from 'preposition'\n",
        "    melted_df = melted_df[melted_df['preposition'] != 'Total']\n",
        "\n",
        "    # Create the plot for prepositions\n",
        "    plot = (\n",
        "        ggplot(melted_df, aes(x='spatial_sense', y='percentage', fill='type'))\n",
        "        + geom_bar(position=\"stack\", stat=\"identity\")\n",
        "        + geom_text(\n",
        "            aes(label='percentage.astype(str) + \"%\"'),\n",
        "            position=position_stack(vjust=0.5),  # Adjust position to stack\n",
        "            size=12,  # Adjust the text size\n",
        "            color=\"white\"\n",
        "        )\n",
        "        + scale_fill_manual(values=[\"lightblue\", \"darkblue\"], labels=[\"Correct\", \"Error\"])\n",
        "        + theme_gray()\n",
        "        + theme(\n",
        "            figure_size=(10, 6),\n",
        "            legend_position='right',\n",
        "            legend_direction='vertical',\n",
        "            legend_title=element_text(text='Type', size=12),\n",
        "            legend_text=element_text(size=10),\n",
        "            axis_text_x=element_text(size=10, angle=45, vjust=1, hjust=-1),  # Adjust x-axis text size\n",
        "            axis_text_y=element_text(size=10),  # Adjust y-axis text size\n",
        "            axis_title_x=element_text(size=12),  # Adjust x-axis title size and weight\n",
        "            axis_title_y=element_text(size=12)  # Adjust y-axis title size and weight\n",
        "        )\n",
        "        + labs(x=\"Meaning\", y=\"Percentage\", )\n",
        "        + facet_grid('~preposition', scales='free')  # Separate by preposition\n",
        "    )\n",
        "    print(plot)\n",
        "\n",
        "# Call the function with the example DataFrame\n",
        "plot_stacked_bar(df)"
      ],
      "metadata": {
        "id": "0wAlwOfgRM5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_stacked_bar(df)"
      ],
      "metadata": {
        "id": "aTM_lkLBaNVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "\n",
        "def plot_stacked_bar_dois(error_counts, label):\n",
        "    \"\"\"\n",
        "    Plots a stacked bar chart based on the provided DataFrame.\n",
        "    \"\"\"\n",
        "    # Group by spatial_sense_label, spatial_sense, and error_type, summing the counts\n",
        "  #  error_counts = df.groupby(['spatial_sense_label', 'spatial_sense', 'error_type'])['count'].sum().reset_index()\n",
        "\n",
        "    # Calculate total errors for each spatial_sense_label\n",
        "  #  total_counts = error_counts.groupby('spatial_sense_label')['count'].sum().reset_index()\n",
        "  #  total_counts = total_counts.rename(columns={'count': 'total_count'})\n",
        "\n",
        "    # Merge total_counts back to error_counts\n",
        " #   error_counts = error_counts.merge(total_counts, on='spatial_sense_label')\n",
        "\n",
        "    # Calculate percentages\n",
        "    error_counts['percentage'] = (error_counts['count'] / error_counts['error_counts']) * 100\n",
        "\n",
        "    # Define 4 blue shades excluding darkblue for highest values\n",
        "    blue_shades = [\"#4682b4\", \"#3C5291\", \"#4B68B8\"]\n",
        "\n",
        "    # Map the unique spatial_sense values to the blue shades\n",
        "    unique_spatial_sense = error_counts['spatial_sense'].unique()\n",
        "    color_mapping = {unique_spatial_sense[i]: blue_shades[i % len(blue_shades)] for i in range(len(unique_spatial_sense))}\n",
        "\n",
        "    # Conditionally assign 'lightblue' to specific values\n",
        "    lightblue_conditions = error_counts['spatial_sense'].isin(['Across(5)', 'Through(5)', 'Into(3)', 'Onto(3)'])\n",
        "    color_mapping.update(dict.fromkeys(error_counts[lightblue_conditions]['spatial_sense'], \"lightblue\"))\n",
        "\n",
        "    # Ensure the highest value gets 'darkblue'\n",
        "    max_spatial_sense = error_counts.loc[error_counts['count'].idxmax(), 'spatial_sense']\n",
        "    color_mapping[max_spatial_sense] = 'darkblue'\n",
        "\n",
        "    # Create the plot\n",
        "    p = (\n",
        "        ggplot(error_counts, aes(x='spatial_sense_label', y='percentage', fill='spatial_sense'))\n",
        "        + geom_bar(position=\"stack\", stat=\"identity\")\n",
        "        + geom_text(\n",
        "            aes(label='round(percentage, 1).astype(str) + \"%\"'),\n",
        "            position=position_stack(vjust=0.5),  # Adjust position to stack\n",
        "            size=12,  # Adjust the text size\n",
        "            color=\"white\"\n",
        "        )\n",
        "        + scale_fill_manual(values=color_mapping)\n",
        "        + theme_gray()\n",
        "        + theme(\n",
        "            figure_size=(10, 6),\n",
        "            legend_position='right',\n",
        "            legend_direction='vertical',\n",
        "            legend_title=element_text(text='Sense', size=10),\n",
        "            legend_text=element_text(size=10),\n",
        "            axis_text_x=element_text(size=10),  # Adjust x-axis text size\n",
        "            axis_text_y=element_text(size=10),  # Adjust y-axis text size\n",
        "        )\n",
        "        + labs(x=\"Preposition\", y=\"Percentage\", title=\"Specific Spatial vs. Non-spatial Error Types by Preposition Sense (%)\")\n",
        "    )\n",
        "\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "M6Kz6aHBSKx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from plotnine import ggplot, aes, geom_bar, geom_text, scale_fill_manual, theme_gray, theme, element_text, labs, position_stack\n",
        "\n",
        "def plot_stacked_bar(error_counts, label):\n",
        "    \"\"\"\n",
        "    Plots a stacked bar chart based on the provided DataFrame.\n",
        "    \"\"\"\n",
        "    # Calculate percentages and round to the nearest integer\n",
        "    error_counts['percentage'] = (error_counts['count'] / error_counts['total_ocurrences']) * 100\n",
        "    error_counts['percentage'] = error_counts['percentage'].fillna(0).astype(int)\n",
        "    error_counts['count'] = error_counts['count'].fillna(0).astype(int)\n",
        "\n",
        "    # Define blue shades excluding darkblue for highest values\n",
        "    blue_shades = [\"#3C5291\"]  # Starting with two shades for remaining assignments\n",
        "\n",
        "    # Initialize color mapping\n",
        "    color_mapping = {}\n",
        "\n",
        "    # Conditionally assign 'lightblue' to specific values\n",
        "    lightblue_conditions = error_counts['spatial_sense'].isin(['Across(5)', 'Through(5)', 'Into(3)', 'Onto(3)'])\n",
        "    color_mapping.update(dict.fromkeys(error_counts[lightblue_conditions]['spatial_sense'], \"lightblue\"))\n",
        "\n",
        "    # Conditionally assign 'darkblue' to highest percentage for each unique spatial_sense_label not in lightblue_conditions\n",
        "    for label in error_counts['spatial_sense_label'].unique():\n",
        "        label_values = error_counts[(error_counts['spatial_sense_label'] == label) & ~lightblue_conditions]\n",
        "        if not label_values.empty:\n",
        "            max_label_value = label_values.loc[label_values['percentage'].idxmax(), 'spatial_sense']\n",
        "            color_mapping[max_label_value] = 'darkblue'\n",
        "\n",
        "    # Conditionally assign '#4682b4' to highest percentage for each unique spatial_sense_label not in lightblue_conditions or darkblue\n",
        "    for label in error_counts['spatial_sense_label'].unique():\n",
        "        label_values = error_counts[(error_counts['spatial_sense_label'] == label) &\n",
        "                                    ~error_counts['spatial_sense'].isin(color_mapping.keys())]\n",
        "        if not label_values.empty:\n",
        "            max_label_value = label_values.loc[label_values['percentage'].idxmax(), 'spatial_sense']\n",
        "            color_mapping[max_label_value] = '#4B68B8'\n",
        "\n",
        "        # Conditionally assign '#4682b4' to highest percentage for each unique spatial_sense_label not in lightblue_conditions or darkblue\n",
        "    for label in error_counts['spatial_sense_label'].unique():\n",
        "        label_values = error_counts[(error_counts['spatial_sense_label'] == label) &\n",
        "                                    ~error_counts['spatial_sense'].isin(color_mapping.keys())]\n",
        "        if not label_values.empty:\n",
        "            max_label_value = label_values.loc[label_values['percentage'].idxmax(), 'spatial_sense']\n",
        "            color_mapping[max_label_value] = '#3C5291' #4682b4\n",
        "\n",
        "    # Assign remaining blue shades to the highest values not in lightblue or darkblue or #4682b4\n",
        "    remaining_values = error_counts[~error_counts['spatial_sense'].isin(color_mapping.keys())]\n",
        "    for index, value in remaining_values.nlargest(len(blue_shades), 'percentage').iterrows():\n",
        "        if value['spatial_sense'] not in color_mapping and blue_shades:\n",
        "            color_mapping[value['spatial_sense']] = blue_shades.pop(0)\n",
        "\n",
        "    # Map any remaining values to a default color if there are not enough blue shades\n",
        "    default_color = \"#B0C4DE\"  # Light steel blue as a default color\n",
        "    for value in error_counts['spatial_sense'].unique():\n",
        "        if value not in color_mapping:\n",
        "            color_mapping[value] = default_color\n",
        "\n",
        "    # Create the plot\n",
        "    p = (\n",
        "        ggplot(error_counts, aes(x='spatial_sense_label', y='percentage', fill='spatial_sense'))\n",
        "        + geom_bar(position=\"stack\", stat=\"identity\")\n",
        "        + geom_text(\n",
        "            aes(label='round(percentage, 1).astype(str) + \"%\"'),\n",
        "          #  aes(label='count'),\n",
        "            position=position_stack(vjust=0.5),  # Adjust position to stack\n",
        "            size=12,  # Adjust the text size\n",
        "            color=\"white\"\n",
        "        )\n",
        "        + scale_fill_manual(values=color_mapping)\n",
        "        + theme_gray()\n",
        "        + theme(\n",
        "            figure_size=(10, 6),\n",
        "            legend_position='right',\n",
        "            legend_direction='vertical',\n",
        "            legend_title=element_text(text='Sense', size=10),\n",
        "            legend_text=element_text(size=10),\n",
        "            axis_text_x=element_text(size=10),  # Adjust x-axis text size\n",
        "            axis_text_y=element_text(size=10),  # Adjust y-axis text size\n",
        "        )\n",
        "        + labs(x=\"Preposition\", y=\"Count\")\n",
        "    )\n",
        "\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "-2u_WqKcz0IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_stacked_bar(df, 'error_type')"
      ],
      "metadata": {
        "id": "tCVYcQHjG8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Lista de erros\n",
        "err = ['sp', 'po', 'ws', 'ie']\n",
        "erros = ['sp', 'po', 'ws']\n",
        "erros2 = ['ie']\n",
        "\n",
        "# Encontrar todos os arquivos CSV no diretÃ³rio\n",
        "file_paths = glob.glob('*.csv')\n",
        "\n",
        "# Inicializar uma lista para armazenar DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Iterar sobre os caminhos dos arquivos\n",
        "for file_path in file_paths:\n",
        "    # Ler o arquivo CSV em um DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Descartar a coluna 'Unnamed: 0' se existir\n",
        "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
        "    # Filtrar o DataFrame para os erros especificados\n",
        "    df_filtered = df[df['error_type'].isin(err)]\n",
        "\n",
        "    # Filter out rows where 'spatial_sense' ends with (5) or (3) if values are 'sp', 'po', 'ws'\n",
        "  #  df_filtered = df_filtered[~((df_filtered['spatial_sense'].str.endswith('(3)') | df_filtered['spatial_sense'].str.endswith('(5)')) & df_filtered['error_type'].isin(erros))]\n",
        "  #  df_filtered = df_filtered[~((df_filtered['spatial_sense'].str.endswith('(1)') |\n",
        "   #                              df_filtered['spatial_sense'].str.endswith('(2)') |\n",
        "   #                              df_filtered['spatial_sense'].str.endswith('(3)') |\n",
        "   #                              df_filtered['spatial_sense'].str.endswith('(4)')) &\n",
        "   #                             (df_filtered['error_type'] == 'ie'))]\n",
        "\n",
        "    # Substituir os valores de 'error_type' conforme necessÃ¡rio\n",
        "    df_filtered.loc[df_filtered['error_type'].isin(erros), 'error_type'] = 'Spatial'\n",
        "    df_filtered.loc[df_filtered['error_type'] == 'ie', 'error_type'] = 'Non-spatial'\n",
        "\n",
        "    df_filtered.loc[df_filtered['spatial_sense'].isin(['Into(3)', 'Onto(3)', 'Across(5)', 'Through(5)']), 'error_type'] = 'Non-spatial'\n",
        "    df_filtered.loc[df_filtered['spatial_sense'].str.endswith('(1)') |\n",
        "                    df_filtered['spatial_sense'].str.endswith('(2)') |\n",
        "                    df_filtered['spatial_sense'].str.endswith('(4)') |\n",
        "                    df_filtered['spatial_sense'].isin(['Across(3)', 'Through(3)']), 'error_type'] = 'Spatial'\n",
        "\n",
        "   # print(df_filtered)\n",
        "\n",
        "    # Adicionar o DataFrame filtrado Ã  lista\n",
        "    dfs.append(df_filtered)\n",
        "\n",
        "# Concatenar todos os DataFrames da lista em um Ãºnico DataFrame\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "combined_df"
      ],
      "metadata": {
        "id": "m3MVd4cGHjWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Directory containing CSV files\n",
        "folder_path = '/content/data/'\n",
        "\n",
        "# Read all CSV files and compile them into a single DataFrame\n",
        "all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
        "\n",
        "# Sum the occurrences of unique values in the 'spatial_sense' column\n",
        "result = df['spatial_sense'].value_counts().reset_index()\n",
        "result.columns = ['spatial_sense', 'count']\n",
        "print(result)\n",
        "\n",
        "# Export the result to a LaTeX file\n",
        "with open('output.tex', 'w') as f:\n",
        "    f.write(\"\\\\begin{longtable}{c c}\\n\")\n",
        "    f.write(\"\\\\caption{Occurrences of Spatial Senses.}\\n\")\n",
        "    f.write(\"\\\\label{tab:spatial_senses} \\\\\\\\\\n\")\n",
        "    f.write(\"\\\\midrule\\n\")\n",
        "    f.write(\"\\\\toprule\\n\")\n",
        "    f.write(\"\\\\textbf{Spatial Sense} & \\\\textbf{Count} \\\\\\\\\\n\")\n",
        "    f.write(\"\\\\midrule\\n\")\n",
        "    f.write(\"\\\\endfirsthead\\n\")\n",
        "    f.write(\"\\\\toprule\\n\")\n",
        "    f.write(\"\\\\textbf{Spatial Sense} & \\\\textbf{Count} \\\\\\\\\\n\")\n",
        "    f.write(\"\\\\midrule\\n\")\n",
        "    f.write(\"\\\\endhead\\n\")\n",
        "    f.write(\"\\\\bottomrule\\n\")\n",
        "    f.write(\"\\\\endfoot\\n\")\n",
        "    f.write(\"\\\\bottomrule\\n\")\n",
        "    f.write(\"\\\\endlastfoot\\n\")\n",
        "\n",
        "    for _, row in result.iterrows():\n",
        "        f.write(f\"{row['spatial_sense']} & {row['count']} \\\\\\\\\\n\")\n",
        "\n",
        "    f.write(\"\\\\end{longtable}\\n\")\n",
        "\n",
        "print(\"LaTeX table exported successfully.\")\n"
      ],
      "metadata": {
        "id": "RWso_eE8oswi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame with a default column name '0'\n",
        "df = pd.read_csv('/content/Phrasal_Verbs_Wikipedia.csv', header=None)\n",
        "\n",
        "# Define the prepositions to filter by\n",
        "prepositions = ['across', 'through', 'into', 'onto']\n",
        "\n",
        "# Initialize a dictionary to store the lists of phrasal verbs for each preposition\n",
        "preposition_phrasal_verbs = {prep: [] for prep in prepositions}\n",
        "\n",
        "# Iterate through each value in the DataFrame\n",
        "for value in df[0]:\n",
        "    # Check if the phrasal verb contains any of the prepositions\n",
        "    for prep in prepositions:\n",
        "        if prep in value.lower():\n",
        "            # If a preposition is found, add the phrasal verb to the corresponding list\n",
        "            preposition_phrasal_verbs[prep].append(value)\n",
        "\n",
        "# Generate LaTeX table code for the prepositions summary\n",
        "latex_summary_table = r\"\"\"\n",
        "\\begin{table}[ht]\n",
        "\\centering\n",
        "\\caption{Prevalence of Prepositions in Phrasal Verbs}\n",
        "\\label{tab:prevalence}\n",
        "\\begin{tabular}{@{}lc@{}}\n",
        "\\toprule\n",
        "\\textbf{Preposition} & \\textbf{Occurrences} \\\\\n",
        "\\midrule\n",
        "\"\"\"\n",
        "\n",
        "for prep, verbs in preposition_phrasal_verbs.items():\n",
        "    latex_summary_table += f\"{prep.capitalize()} & {len(verbs)} \\\\\\\\\\n\"\n",
        "\n",
        "latex_summary_table += r\"\"\"\n",
        "\\bottomrule\n",
        "\\end{tabular}\n",
        "\\end{table}\n",
        "\"\"\"\n",
        "\n",
        "# Save the summary LaTeX table to a file\n",
        "with open('preposition_summary_table.tex', 'w') as file:\n",
        "    file.write(latex_summary_table)\n",
        "\n",
        "# Generate LaTeX table codes for each preposition list\n",
        "for prep, verbs in preposition_phrasal_verbs.items():\n",
        "    latex_list_table = r\"\"\"\n",
        "\\begin{table}[ht]\n",
        "\\centering\n",
        "\\caption{Phrasal Verbs Containing '\"\"\" + prep.capitalize() + r\"\"\"'}\n",
        "\\label{tab:\"\"\" + prep + r\"\"\"}\n",
        "\\begin{tabular}{@{}l@{}}\n",
        "\\toprule\n",
        "\\textbf{Phrasal Verb} \\\\\n",
        "\\midrule\n",
        "\"\"\"\n",
        "    for verb in verbs:\n",
        "        latex_list_table += f\"{verb} \\\\\\\\\\n\"\n",
        "\n",
        "    latex_list_table += r\"\"\"\n",
        "\\bottomrule\n",
        "\\end{tabular}\n",
        "\\end{table}\n",
        "\"\"\"\n",
        "\n",
        "    # Save each preposition list LaTeX table to a file\n",
        "    with open(f'{prep}_phrasal_verbs_table.tex', 'w') as file:\n",
        "        file.write(latex_list_table)"
      ],
      "metadata": {
        "id": "n0E6EfEk-abu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "\n",
        "# Error type substitutions\n",
        "error_substitutions = {\n",
        "    \"cc\": \"Correct\",\n",
        "    \"un\": \"Untranslated\",\n",
        "    \"om\": \"Omission\",\n",
        "    \"re\": \"Repetition\",\n",
        "    \"ad\": \"Addition\",\n",
        "    \"ag\": \"Agreement\",\n",
        "    \"co\": \"Collocation\",\n",
        "    \"wl\": \"Wrong Lexis\",\n",
        "    \"wt\": \"Wrong Mood/\\nTense\",\n",
        "    \"an\": \"Anglicism\",\n",
        "    \"gr\": \"Grammar/\\northography\",\n",
        "    \"in\": \"Interlanguage/\\ncode-switching\",\n",
        "    \"sp\": \"Syntactic\\nProjection\",\n",
        "    \"po\": \"Polysemy\",\n",
        "    \"ws\": \"Wrong Sense\",\n",
        "    \"ie\": \"Idiomatic\\nExpression\"\n",
        "}\n",
        "\n",
        "# Melt the dataframe into a format that can be used with ggplot\n",
        "df_melted = pd.melt(df, id_vars=['Model'], var_name='Error Type', value_name='Count')\n",
        "\n",
        "# Replace error types with their substitutions\n",
        "df_melted['Error Type'] = df_melted['Error Type'].replace(error_substitutions)\n",
        "\n",
        "# Convert counts to integers\n",
        "df_melted['Count'] = df_melted['Count'].astype(int)\n",
        "\n",
        "# Create the ggplot object\n",
        "p = (\n",
        "    ggplot(df_melted, aes(x='Error Type', y='Model', fill='Count'))\n",
        "    + geom_tile()\n",
        "    + geom_text(aes(label='Count'), color=\"white\", size=10)\n",
        "    + scale_fill_gradient(low=\"lightblue\", high=\"darkblue\")\n",
        "    + coord_fixed()\n",
        "    + theme_gray()\n",
        "    + theme(\n",
        "        figure_size=(10, 6),\n",
        "        axis_text_x=element_text(angle=45, vjust=1, hjust=-5, size=9),\n",
        "        legend_position='right',\n",
        "        legend_direction='vertical',\n",
        "        legend_title=element_text(size=10),\n",
        "        legend_text=element_text(size=10),\n",
        "    )\n",
        "    + xlab(\"Error Type\") + ylab(\"Model\")\n",
        ")\n",
        "\n",
        "# Print the plot\n",
        "print(p)"
      ],
      "metadata": {
        "id": "NA3olbJn3OMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w23Oq3TiDQ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cLJXvc4fDcqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UtHeOis4DYii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_by_spatial_sense['error_type'].unique()"
      ],
      "metadata": {
        "id": "HwfARbxml729"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_by_model = group_by_model_and_error(folder_path)\n",
        "result_by_model"
      ],
      "metadata": {
        "id": "ACOnLWVddniq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_heatmap(df, \"spatial_sense\")"
      ],
      "metadata": {
        "id": "ab3aneIAdwSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_heatmap(result_by_model, \"Model\")"
      ],
      "metadata": {
        "id": "xophYgG4etp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from plotnine import ggplot, aes, geom_tile, geom_text, theme_minimal\n",
        "\n",
        "# Define the data for the contingency table\n",
        "data = {\n",
        "    \"Category1\": [\"A\", \"A\", \"B\", \"B\"],\n",
        "    \"Category2\": [\"X\", \"Y\", \"X\", \"Y\"],\n",
        "    \"Count\": [10, 15, 20, 25]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a ggplot object\n",
        "p = (\n",
        "    ggplot(df, aes(x='Category1', y='Category2', fill='Count')) +  # Define aesthetics\n",
        "    geom_tile(color='black') +  # Add tiles\n",
        "    geom_text(aes(label='Count'), size=12) +  # Add text labels\n",
        "    theme_minimal()  # Apply a minimal theme\n",
        ")\n",
        "\n",
        "# Print the plot\n",
        "print(p)"
      ],
      "metadata": {
        "id": "saL2NdOX_ao6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Contingency table based on the number of errors\n",
        "data = np.array([\n",
        "    [7, 25],\n",
        "    [112, 55],\n",
        "    [8, 3],\n",
        "    [73, 32]\n",
        "])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "sFyByA4cZ77K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "#ACROSS\n",
        "\n",
        "# Contingency table based on the number of errors\n",
        "data = np.array([\n",
        "    [25, 7],\n",
        "    [45, 14]\n",
        "])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "_jeCwYTLPWV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INTO\n",
        "\n",
        "# Contingency table based on the number of errors\n",
        "data = np.array([\n",
        "    [55, 112],\n",
        "    [134, 497]\n",
        "])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "DWg1F0IRQRco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ONTO\n",
        "\n",
        "# Contingency table based on the number of errors\n",
        "data = np.array([\n",
        "    [3, 8],\n",
        "    [18, 20]\n",
        "])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "yAzWzg_hRZNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THROUGH\n",
        "\n",
        "# Contingency table based on the number of errors\n",
        "data = np.array([\n",
        "    [32, 73],\n",
        "    [101, 263]\n",
        "])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "1ErN4pRabGYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOTAL\n",
        "\n",
        "# Contingency table based on the number of errors\n",
        "data = np.array([\n",
        "    [787, 298],\n",
        "    [200, 115]\n",
        "])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "lXC762tLslEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ggplot"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cSTt03dU6azt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Actual values\n",
        "actual_values = np.array([\n",
        "    [115, 200],\n",
        "    [298, 787]\n",
        "])\n",
        "\n",
        "# Expected frequencies\n",
        "expected_frequencies = np.array([\n",
        "    [92.925, 222.075],\n",
        "    [320.075, 764.925]\n",
        "])\n",
        "\n",
        "# Flatten the arrays and round the values\n",
        "actual_values_flat = actual_values.flatten()\n",
        "expected_frequencies_flat = expected_frequencies.flatten()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Categories': ['Errors', 'Errors', 'Corrects', 'Corrects', 'Errors', 'Errors', 'Corrects', 'Corrects'],\n",
        "    'Subcategory': ['Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial'],\n",
        "    'Frequency': np.concatenate([actual_values_flat, expected_frequencies_flat]).round(6),\n",
        "    'Type': ['Actual', 'Actual', 'Actual', 'Actual', 'Expected', 'Expected', 'Expected', 'Expected']\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "5UEbsyYRSxLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "import pandas as pd\n",
        "\n",
        "# Labels for the groups\n",
        "categories = ['Errors', 'Corrects']\n",
        "subcategories = ['Spatial', 'Non-spatial']\n",
        "\n",
        "# Actual values\n",
        "actual_values = np.array([\n",
        "    [115, 200],\n",
        "    [298, 787]\n",
        "])\n",
        "\n",
        "# Expected frequencies\n",
        "expected_frequencies = np.array([\n",
        "    [92.925, 222.075],\n",
        "    [320.075, 764.925]\n",
        "])\n",
        "\n",
        "# Flatten the arrays and round the values\n",
        "actual_values_flat = actual_values.flatten()\n",
        "expected_frequencies_flat = expected_frequencies.flatten()\n",
        "\n",
        "# Labels for the groups\n",
        "categories = ['Corrects', 'Corrects', 'Corrects', 'Errors', 'Errors', 'Errors']\n",
        "subcategories = ['Onto(i)', 'Onto(ii)', 'Onto(iii)', 'Onto(i)', 'Onto(ii)', 'Onto(iii)']\n",
        "\n",
        "# Actual values\n",
        "actual_values = np.array([\n",
        "    [12, 6, 20],\n",
        "    [14, 7, 28]\n",
        "])\n",
        "\n",
        "# Expected frequencies\n",
        "expected_frequencies = np.array([\n",
        "    [11.35632184, 5.67816092, 20.96551724],\n",
        "    [14.64367816, 7.32183908, 27.03448276]\n",
        "])\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Iterate over categories, subcategories, actual values, and expected frequencies\n",
        "for cat, subcat, actual, expected in zip(categories, subcategories, actual_values, expected_frequencies):\n",
        "    # Add actual data for each subcategory\n",
        "    for act in actual:\n",
        "        data.append({'Categories': cat, 'Subcategory': subcat, 'Frequency': act, 'Type': 'Actual'})\n",
        "    # Add expected data for each subcategory\n",
        "    for exp in expected:\n",
        "        data.append({'Categories': cat, 'Subcategory': subcat, 'Frequency': exp, 'Type': 'Expected'})\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Round the Frequency values to two decimal places\n",
        "df['Frequency'] = df['Frequency'].round(2)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n",
        "# Plot using plotnine\n",
        "gg = (\n",
        "    ggplot(df, aes(x='Categories', y='Frequency', fill='Type')) +\n",
        "    geom_bar(stat='identity', position='dodge') +\n",
        "    facet_wrap('~Subcategory', scales='free') +  # Separate graphs by subcategory\n",
        "    labs(x='Categories', y='Frequencies') + #, title='ACROSS: Actual vs. Expected Frequencies') +\n",
        "    scale_fill_manual(values=[\"darkblue\", \"lightblue\"], labels=[\"Observed\", \"Expected\"])  # Set custom fill colors and labels\n",
        ")\n",
        "\n",
        "print(gg)"
      ],
      "metadata": {
        "id": "-uqasN631C9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the groups\n",
        "categories = (['Corrects', 'Corrects'] + ['Errors', 'Errors']) * 2\n",
        "subcategories = ['Non-Spatial', 'Non-Spatial', 'Spatial', 'Spatial'] * 2\n",
        "typep = (['Actual', 'Actual']* 2) + (['Expected', 'Expected']) * 2\n",
        "\n",
        "\n",
        "# Actual values\n",
        "actual_values = np.array([\n",
        "    [787, 298],\n",
        "    [200, 115]\n",
        "])\n",
        "\n",
        "# Expected frequencies\n",
        "expected_frequencies = np.array([\n",
        "    [764.925, 320.075],\n",
        "    [222.075, 92.925]\n",
        "])\n",
        "\n",
        "# Flatten the arrays and round the values\n",
        "actual_values_flat = actual_values.flatten()\n",
        "expected_frequencies_flat = expected_frequencies.flatten()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Categories': ['Corrects', 'Corrects', 'Errors', 'Errors', 'Corrects', 'Corrects', 'Errors', 'Errors'],\n",
        "    'Subcategory': ['Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial'],\n",
        "    'Frequency': np.concatenate([actual_values_flat, expected_frequencies_flat]).round(6),\n",
        "    'Type': ['Observed', 'Observed', 'Observed', 'Observed', 'Expected', 'Expected', 'Expected', 'Expected']\n",
        "})\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Round the Frequency values to two decimal places\n",
        "df['Frequency'] = df['Frequency'].round(2)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n",
        "# Plot using plotnine\n",
        "gg = (\n",
        "    ggplot(df, aes(x='Categories', y='Frequency', fill='Type')) +\n",
        "    geom_bar(stat='identity', position='dodge') +\n",
        "    geom_text(aes(label='Frequency'), color='white', position=position_dodge(width=0.9), size=10, va='top') +  # Add text labels\n",
        "    facet_wrap('~Subcategory', scales='free') +  # Separate graphs by subcategory\n",
        "    labs(x='Categories', y='Frequency') +  # , title='ACROSS: Actual vs. Expected Frequencies'\n",
        "    scale_fill_manual(values=[\"darkblue\", \"lightblue\"], labels=[\"Observed\", \"Expected\"])  # Set custom fill colors and labels\n",
        ")\n",
        "\n",
        "print(gg)\n"
      ],
      "metadata": {
        "id": "rIeogZHhTaCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import scale_fill_manual, theme_gray, element_text, theme, ggplot, position_dodge, aes, geom_bar, geom_tile, geom_text, theme_minimal, scale_fill_gradient, labs, facet_wrap\n",
        "\n",
        "# Actual values\n",
        "actual_values = np.array([\n",
        "    [787, 298],\n",
        "    [200, 115]\n",
        "])\n",
        "\n",
        "# Expected frequencies\n",
        "expected_frequencies = np.array([\n",
        "    [764.925, 320.075],\n",
        "    [222.075, 92.925]\n",
        "])\n",
        "\n",
        "# Flatten the arrays and round the values\n",
        "actual_values_flat = actual_values.flatten()\n",
        "expected_frequencies_flat = expected_frequencies.flatten()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Categories': ['Corrects', 'Corrects', 'Errors', 'Errors', 'Corrects', 'Corrects', 'Errors', 'Errors'],\n",
        "    'Subcategory': ['Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial', 'Non-Spatial', 'Spatial'],\n",
        "    'Frequency': np.concatenate([actual_values_flat, expected_frequencies_flat]).round(6),\n",
        "    'Type': ['Observed', 'Observed', 'Observed', 'Observed', 'Expected', 'Expected', 'Expected', 'Expected']\n",
        "})\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Plot using plotnine\n",
        "from plotnine import ggplot, aes, geom_tile, geom_text, theme_minimal, scale_fill_gradient, facet_wrap\n",
        "\n",
        "# Define the base plot\n",
        "#base_plot = (ggplot(df, aes(x='Subcategory', y='Categories', fill='Frequency')) +\n",
        "#             geom_tile() +\n",
        "#             geom_text(aes(label='Frequency'), color='white') +\n",
        " #            scale_fill_gradient(low='lightblue', high='darkblue') +\n",
        "#             theme_gray()\n",
        "           #  labs(title='Chi-Square Test of Independence', x='', y='')\n",
        " #          )\n",
        "\n",
        "# Facet the plot to separate Actual and Expected\n",
        "#plot = base_plot + facet_wrap('~Type')\n",
        "\n",
        "# Display the plot\n",
        "#print(plot)\n",
        "\n",
        "\n",
        "\n",
        "# Bar plot to show observed vs expected\n",
        "bar_plot = (\n",
        "    ggplot(df, aes(x='Categories', y='Frequency', fill='Type')) +\n",
        "    #geom_bar(stat='identity', position='dodge') +\n",
        "    geom_bar(stat='identity', position=position_dodge(width=0.9)) +\n",
        "    geom_text(aes(label='Frequency'), position=position_dodge(width=0.9), size=8, va='bottom') +\n",
        "    facet_wrap('~Subcategory', scales='free') +  # Separate graphs by subcategory\n",
        "    labs(y='Count', fill='Type') +\n",
        "    theme_gray() +\n",
        "    scale_fill_manual(values=[\"lightblue\", \"darkblue\"], labels=[\"Observed\", \"Expected\"])  # Set custom fill colors and labels\n",
        "\n",
        "   # theme(axis_text_x=element_text(rotation=45, hjust=1))\n",
        ")\n",
        "\n",
        "print(bar_plot)\n"
      ],
      "metadata": {
        "id": "s50-xSNgOYIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from plotnine import ggplot, aes, geom_bar, geom_text, labs, theme_minimal, geom_line, geom_vline, annotate, theme, element_text, position_dodge, geom_area\n",
        "\n",
        "# Data for observed and expected absences\n",
        "data = {\n",
        "    'Day': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
        "    'Observed': [23, 16, 14, 19, 28],\n",
        "    'Expected': [20, 20, 20, 20, 20]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bar plot to show observed vs expected\n",
        "bar_plot = (\n",
        "    ggplot(df.melt(id_vars='Day'), aes(x='Day', y='value', fill='variable')) +\n",
        "    geom_bar(stat='identity', position=position_dodge(width=0.9)) +\n",
        "    geom_text(aes(label='value'), position=position_dodge(width=0.9), size=8, va='bottom') +\n",
        "    labs(title='Observed vs Expected Absences', y='Count', fill='Type') +\n",
        "    theme_minimal() +\n",
        "    theme(axis_text_x=element_text(rotation=45, hjust=1))\n",
        ")\n",
        "\n",
        "# Data for chi-square distribution plot\n",
        "x = np.linspace(0, 20, 400)\n",
        "y = stats.chi2.pdf(x, df=4)  # degrees of freedom = 4\n",
        "\n",
        "# Critical value for chi-square with df=4 and alpha=0.05\n",
        "critical_value = stats.chi2.ppf(0.95, df=3)\n",
        "\n",
        "# DataFrame for the chi-square distribution\n",
        "chi_square_df = pd.DataFrame({'x': x, 'y': y})\n",
        "\n",
        "# Plot chi-square distribution\n",
        "chi_square_plot = (\n",
        "    ggplot(chi_square_df, aes(x='x', y='y')) +\n",
        "    geom_line(color='black') +\n",
        "    geom_vline(xintercept=critical_value, color='red', linetype='dashed') +\n",
        "    geom_area(aes(x='x', y='y'), data=chi_square_df[chi_square_df['x'] >= critical_value], fill='red', alpha=0.3) +\n",
        "    geom_area(aes(x='x', y='y'), data=chi_square_df[chi_square_df['x'] < critical_value], fill='lightblue', alpha=0.3) +\n",
        "    annotate('text', x=critical_value + 2.2, y=max(y)/2, label=f'Critical value\\n{round(critical_value, 2)}', color='red') +\n",
        "    annotate('text', x=critical_value - 5, y=max(y)/2, label='Do Not Reject', color='black') +\n",
        "    annotate('text', x=critical_value + 3, y=0.025, label='Reject', color='red') +\n",
        "    annotate('text', x=17, y=max(y) * 0.9, label=r'$\\alpha = 0.05$', color='black') +\n",
        "    annotate('text', x=17, y=max(y) * 0.7, label='df = 3', color='black', size=10) +  # Annotate degrees of freedom\n",
        "    labs(x='$\\chi^2$', y='Density') +\n",
        "    theme_minimal()\n",
        ")\n",
        "\n",
        "# Display the plots\n",
        "print(bar_plot)\n",
        "print(chi_square_plot)"
      ],
      "metadata": {
        "id": "XtKkKiTMi_wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd"
      ],
      "metadata": {
        "id": "tDNr6E_WgYA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}